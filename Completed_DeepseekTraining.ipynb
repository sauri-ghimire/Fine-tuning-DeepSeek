{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12094127,"sourceType":"datasetVersion","datasetId":7613402}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:56:52.846070Z","iopub.execute_input":"2025-06-08T04:56:52.846235Z","iopub.status.idle":"2025-06-08T04:56:52.965286Z","shell.execute_reply.started":"2025-06-08T04:56:52.846213Z","shell.execute_reply":"2025-06-08T04:56:52.964480Z"}},"outputs":[{"name":"stdout","text":"fulldatasett\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls /kaggle/input/fulldatasett","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:57:22.264703Z","iopub.execute_input":"2025-06-08T04:57:22.265490Z","iopub.status.idle":"2025-06-08T04:57:22.392430Z","shell.execute_reply.started":"2025-06-08T04:57:22.265456Z","shell.execute_reply":"2025-06-08T04:57:22.391745Z"}},"outputs":[{"name":"stdout","text":"traindeepseek_augmented.json\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q transformers datasets peft evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:57:44.404005Z","iopub.execute_input":"2025-06-08T04:57:44.404799Z","iopub.status.idle":"2025-06-08T04:59:00.168593Z","shell.execute_reply.started":"2025-06-08T04:57:44.404766Z","shell.execute_reply":"2025-06-08T04:59:00.167821Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#cell 1\nimport os\nimport json\nimport torch\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    default_data_collator\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\n# 1) Point at your JSON file in the Kaggle â€œaugmentedtrainingâ€ dataset:\nJSON_PATH = \"/kaggle/input/fulldatasett/traindeepseek_augmented.json\"\n\n# 2) Base DeepSeek model & example prompts to check â€œbeforeâ€ vs. â€œafterâ€\nMODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\nPROMPTS = [\n    \"### Instruction:\\nHow do I reset my password?\\n\\n### Response:\\n\",\n    \"### Instruction:\\nWhat is refund policy of saipal?\\n\\n### Response:\\n\",\n    \"### Instruction:\\nWhat are Saipalâ€™s opening and closing hours?\\n\\n### Response:\\n\",\n    \"### Instruction:\\nWhat is address of saipal?\\n\\n### Response:\\n\",\n    \"### Instruction:\\nHow can I contact Saipal?\\n\\n### Response:\\n\"\n]\n\n# 3) Run on GPU\nDEVICE = \"cuda\"\n\n# 4) Sanity-check that the JSON file exists\nassert os.path.exists(JSON_PATH), f\"Missing JSON at: {JSON_PATH}\"\nprint(\" Found JSON at:\", JSON_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T05:01:13.567180Z","iopub.execute_input":"2025-06-08T05:01:13.567508Z","iopub.status.idle":"2025-06-08T05:01:40.275511Z","shell.execute_reply.started":"2025-06-08T05:01:13.567482Z","shell.execute_reply":"2025-06-08T05:01:40.274831Z"}},"outputs":[{"name":"stderr","text":"2025-06-08 05:01:27.506861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749358887.710305      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749358887.770467      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":" Found JSON at: /kaggle/input/fulldatasett/traindeepseek_augmented.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#cell 3\n# 1) Read the JSON into Python\nwith open(JSON_PATH, \"r\") as f:\n    all_data = json.load(f)\n\n# 2) Separate â€œtrainâ€ vs. â€œtestâ€ based on the â€œsplitâ€ field\ntrain_data = [ex for ex in all_data if ex.get(\"split\") == \"train\"]\ntest_data  = [ex for ex in all_data if ex.get(\"split\") == \"test\"]\n\n# 3) Wrap them into Hugging Face Datasets\ntrain_ds = Dataset.from_list(train_data)\ntest_ds  = Dataset.from_list(test_data)\n\n# 4) Show counts\nprint(f\"Train examples: {len(train_ds)}\")\nprint(f\"Test examples: {len(test_ds)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T05:01:59.823346Z","iopub.execute_input":"2025-06-08T05:01:59.824479Z","iopub.status.idle":"2025-06-08T05:01:59.853748Z","shell.execute_reply.started":"2025-06-08T05:01:59.824440Z","shell.execute_reply":"2025-06-08T05:01:59.853226Z"}},"outputs":[{"name":"stdout","text":"Train examples: 2000\nTest examples: 300\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#cell 4\nfrom transformers import AutoTokenizer\n\n# 1) Combine instruction+response into â€œtextâ€\nTEMPLATE = \"### Instruction:\\n{input}\\n\\n### Response:\\n{output}\"\ndef format_fn(ex):\n    return {\"text\": TEMPLATE.format(input=ex[\"instruction\"], output=ex[\"response\"])}\n\n# Remove the original columns exactly once here:\ntrain_ds = train_ds.map(format_fn, remove_columns=[\"instruction\",\"response\",\"split\"])\ntest_ds  = test_ds.map(format_fn,  remove_columns=[\"instruction\",\"response\",\"split\"])\n\n# 2) Load tokenizer & set pad_token = eos_token\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# 3) Tokenize (padding to max_length=512)\ndef tok_fn(ex):\n    return tokenizer(\n        ex[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n\ntrain_ds = train_ds.map(tok_fn, batched=True)\ntest_ds  = test_ds.map(tok_fn,  batched=True)\n\n# 4) Quick peek at one entry\nprint(train_ds[0])\n# Should show keys: 'text', 'input_ids', 'attention_mask'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T05:02:30.655898Z","iopub.execute_input":"2025-06-08T05:02:30.656856Z","iopub.status.idle":"2025-06-08T05:02:32.779551Z","shell.execute_reply.started":"2025-06-08T05:02:30.656829Z","shell.execute_reply":"2025-06-08T05:02:32.778855Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"849b9ba49e0b49fd88411274bbfb4d45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795681e66ed9491097a47b9f19c44401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282cb0ab4ad146bcac58deaaa537fff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dca18637fa543d49ded1be727bd8a16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a743bcf58f4179b801b34dd83d0e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1dfdbfd34c40fca3e9ea53c13b5a70"}},"metadata":{}},{"name":"stdout","text":"{'text': '### Instruction:\\nHow do I contact Saipal support?\\n\\n### Response:\\nCall us on +977-1-YAK-MILK or email yaks@saipal.ai.', 'input_ids': [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 14374, 29051, 510, 4340, 653, 358, 3645, 15854, 573, 278, 1824, 1939, 14374, 5949, 510, 7220, 601, 389, 488, 24, 22, 22, 12, 16, 29137, 11907, 5251, 1715, 42, 476, 2551, 379, 9810, 31, 9081, 573, 278, 40383, 13], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#cell 5\ndef add_labels(ex):\n    ex[\"labels\"] = ex[\"input_ids\"]\n    return ex\n\ntrain_ds = train_ds.map(add_labels, batched=False)\ntest_ds  = test_ds.map(add_labels,  batched=False)\n\n# Sanity check: now each example has 'labels'\nprint(train_ds[0].keys())  # should include 'text', 'input_ids', 'attention_mask', 'labels'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T05:02:55.819692Z","iopub.execute_input":"2025-06-08T05:02:55.820031Z","iopub.status.idle":"2025-06-08T05:02:56.806547Z","shell.execute_reply.started":"2025-06-08T05:02:55.819969Z","shell.execute_reply":"2025-06-08T05:02:56.806007Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77bca5302c6a4685a8954c17e922e226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d1e877f335740f78a1461cbfae19e89"}},"metadata":{}},{"name":"stdout","text":"dict_keys(['text', 'input_ids', 'attention_mask', 'labels'])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# â”€â”€ Cell 6: Load DeepSeek-1.5B + attach LoRA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfrom transformers import AutoModelForCausalLM\n\n# 1) Load DeepSeek-1.5B in FP16 and force it all onto a single GPU (DEVICE=\"cuda\" â†’ cuda:0)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True,\n    device_map={\"\": DEVICE}   # â† pin every layer onto cuda:0\n)\nbase_model.gradient_checkpointing_enable()\n\n# 2) Attach tiny LoRA adapters\nlora_cfg = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\npeft_model = get_peft_model(base_model, lora_cfg)\n# (No need to call peft_model.to(DEVICE) because device_map={\"\": DEVICE} already placed it on cuda:0)\n\n# 3) â€œBefore fine-tuningâ€ inference (using the raw base_model)\nprint(\"\\nâ”€â”€ Before fine-tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\nfor prompt in PROMPTS:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    gen_ids = base_model.generate(\n        **inputs,\n        max_new_tokens=100,\n        use_cache=False,\n        do_sample=False\n    )\n    raw_out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n    question = prompt.splitlines()[1]  # e.g. \"How do I reset my password?\"\n    print(f\"ğŸ›  Before fine-tuning ({question}):\\n\",\n          raw_out.split(\"### Response:\")[-1].strip(), \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T05:03:27.983030Z","iopub.execute_input":"2025-06-08T05:03:27.983355Z","iopub.status.idle":"2025-06-08T05:04:35.450632Z","shell.execute_reply.started":"2025-06-08T05:03:27.983334Z","shell.execute_reply":"2025-06-08T05:04:35.450016Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003a0e4bb0724a8085ede3bf4bb400ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40390c879fc844efaf86da318674af17"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44e2e3168b0340e6b8e95494a10b5662"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nâ”€â”€ Before fine-tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ›  Before fine-tuning (How do I reset my password?):\n To reset your password, you can follow these steps:\n\n1. **Log in** to your account using your credentials.\n2. **Go to the account settings**.\n3. **Look for a \"Forgot Password\" or \"Reset Password\" feature**.\n4. **Enter your new password**.\n5. **Confirm the new password**.\n6. **Save your changes**.\n\nIf you encounter any issues, you can contact your account administrator for assistance.\n</think>\n\nTo reset your password, \n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ›  Before fine-tuning (What is refund policy of saipal?):\n The refund policy for Saipal is designed to ensure that customers receive a full refund for any product returned within 7 days of purchase. This policy includes a 100% refund on all returned items, with no conditions or exceptions. Additionally, the policy guarantees a 30-day warranty period, offering free repairs and replacements for any defects. The customer can initiate the return process through the online portal, and there are no hidden fees or additional charges. The policy also includes a 2 \n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ›  Before fine-tuning (What are Saipalâ€™s opening and closing hours?):\n The user has asked about Saipalâ€™s opening and closing hours, but I don't have specific information about that. I can provide general hours for a typical business, but I can't give precise details about Saipal. If you have more context or specific details, I can help further.\n</think>\n\nThe user has asked about Saipalâ€™s opening and closing hours, but I don't have specific information about that. I can provide general hours for a typical business, but I can't \n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ›  Before fine-tuning (What is address of saipal?):\n The address of Saipal is [address].\n\nNote: The response should be in the form of a sentence, using the address of Saipal as the subject and [address] as the predicate.\n\nThe response should be in English, and the address of Saipal should be in the form of a sentence.\n\nThe response should be in the form of a sentence, using the address of Saipal as the subject and [address] as the predicate.\n\nThe response should be in English, \n\nğŸ›  Before fine-tuning (How can I contact Saipal?):\n I can't provide information on specific individuals, but I can help answer general questions about them or provide details about their work.\n\n</think>\n\nI don't have specific information about individuals unless they have been mentioned in the context of the conversation. However, I can provide general information about them or discuss their work if that's relevant. Let me know how I can assist you! \n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, default_data_collator\n\n# 1) Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=1,    # tiny batch so it fits in 1Ã—T4\n    gradient_accumulation_steps=1,\n    num_train_epochs=3,               # you can adjust up or down\n    learning_rate=1e-5,               # very low LR to avoid overfitting\n    logging_steps=100,\n    save_strategy=\"no\",               # weâ€™ll skip intermediate checkpoints\n    fp16=True,                        # use half-precision on GPU\n    report_to=\"none\"\n)\n\n# 2) Instantiate Trainer\ntrainer = Trainer(\n    model=peft_model,                      # from Cell 6\n    args=training_args,\n    train_dataset=train_ds,                # from Cell 4\n    eval_dataset=(test_ds if len(test_ds) > 0 else None),  # from Cell 4\n    tokenizer=tokenizer,                   # from Cell 4\n    data_collator=default_data_collator\n)\n\nprint(\"\\nâ”€â”€ Fine-tuning in progress (GPU, FP16) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\ntrainer.train()\nprint(\"\\n Fine-tuning completed.\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T05:05:32.879422Z","iopub.execute_input":"2025-06-08T05:05:32.879717Z","iopub.status.idle":"2025-06-08T05:44:12.683628Z","shell.execute_reply.started":"2025-06-08T05:05:32.879698Z","shell.execute_reply":"2025-06-08T05:44:12.682866Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3443542844.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"\nâ”€â”€ Fine-tuning in progress (GPU, FP16) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6000/6000 38:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.463600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.400400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.365100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.338400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.276900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.252600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.243500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.234400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.224000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.211000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.196900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.196500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.191300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.186800</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.177600</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.172000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.170300</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.165900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.168600</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.160000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.150300</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.151800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.147900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.144700</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.144100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.144800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.139300</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.138500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.132500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.128100</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.130900</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.127800</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.132000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.122900</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.121200</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.118900</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.119300</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.118000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.124100</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.117900</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.111600</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.109700</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.108800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.112100</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.107600</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.111700</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.108800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.105200</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.105600</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.105500</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.108000</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.108600</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.103100</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.108500</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.105800</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.106700</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.106800</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.101500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n Fine-tuning completed.\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 8: Save LoRA adapters and generate â€œafter fine-tuningâ€\n\n# 1) First, save the fine-tuned LoRA weights to a known directory:\npeft_model.save_pretrained(\"/kaggle/working/deepseek-lora\")\n\n# 2) Now reload the base + LoRA adapters from that directory:\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\n\nbase = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True,\n    device_map={\"\": DEVICE}   # pin everything onto cuda:0\n)\nmodel = PeftModel.from_pretrained(base, \"/kaggle/working/deepseek-lora\")\nmodel.to(DEVICE)\n\n# 3) Greedy generation (do_sample=False) to compare â€œafter fine-tuningâ€ outputs:\nprint(\"\\nâ”€â”€ After fine-tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\nfor prompt in PROMPTS:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    gen_ids = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        use_cache=False,\n        do_sample=False\n    )\n    out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n    question = prompt.splitlines()[1]\n    print(f\" After fine-tuning ({question}):\\n\",\n          out.split(\"### Response:\")[-1].strip(), \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T05:45:05.942305Z","iopub.execute_input":"2025-06-08T05:45:05.943138Z","iopub.status.idle":"2025-06-08T05:45:15.714194Z","shell.execute_reply.started":"2025-06-08T05:45:05.943099Z","shell.execute_reply":"2025-06-08T05:45:15.713476Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nâ”€â”€ After fine-tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" After fine-tuning (How do I reset my password?):\n Go to 'Settings' > 'Security' > 'Reset Password' and enter your email. \n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" After fine-tuning (What is refund policy of saipal?):\n Our refund policy is simple: if you need to return items, we'll process within 24 hours. \n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" After fine-tuning (What are Saipalâ€™s opening and closing hours?):\n Our hours are from 5â€¯AM to 5â€¯PM every day, except on Sundays and Mondays where we close at 8â€¯AM and 8â€¯PM, respectively. \n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" After fine-tuning (What is address of saipal?):\n We offer cloud services to bring AI to the top of the world. \n\n After fine-tuning (How can I contact Saipal?):\n Reach support via email at support@saipal.ai or call +977-123-456-789-123. \n\n","output_type":"stream"}],"execution_count":12}]}